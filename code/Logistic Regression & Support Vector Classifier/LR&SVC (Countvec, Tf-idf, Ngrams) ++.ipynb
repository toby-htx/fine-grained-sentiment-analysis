{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Similar to the LR&SVC (Countvec, Tf-idf, Ngrams) notebook, but with the creation and addition of 2 features (No of Adj & No of Adv) for the LR and SVC models to train on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "oMqGtAVOFDJY",
        "outputId": "f4f7ce69-e33b-4a67-bf4a-063a9ad5efd0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('LOCAL_PATH_TO_DATASET')\n",
        "df = df[['Emotion','Statement']]\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk4fsyRGFKHj"
      },
      "outputs": [],
      "source": [
        "def process_text(document):\n",
        "     \n",
        "    # Remove extra white space from text\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "         \n",
        "    # Remove all the special characters from text\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        " \n",
        "    # Remove all single characters from text\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        " \n",
        "    # Word tokenization       \n",
        "    tokens = document.split()\n",
        "      \n",
        "    lemma_txt = [stemmer.lemmatize(word) for word in tokens]\n",
        "    # Remove Drop words \n",
        "    lemma_no_stop_txt = [word for word in lemma_txt if word not in en_stop]\n",
        "        \n",
        "    tokens = [word for word in tokens if len(word) > 3]\n",
        "                 \n",
        "    clean_txt = ' '.join(lemma_no_stop_txt)\n",
        " \n",
        "    return clean_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDCgojHQ4n_U"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weight = class_weight.compute_class_weight('balanced'\n",
        "                                               ,np.unique(df['Emotion'])\n",
        "                                               ,df['Emotion'])\n",
        "y = df.pop('Emotion')\n",
        "X = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFgpoKVRFKj5",
        "outputId": "cb4cfb11-2fc8-450d-ab26-eb53a874ebb8"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import re\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "stemmer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')  \n",
        "\n",
        "df['preprocessedStatement'] = df.Statement.apply(process_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roDto_ie5z_0"
      },
      "outputs": [],
      "source": [
        "clean_corpus = df.preprocessedStatement.tolist()\n",
        "clean_corpus[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For the next 3 cells, run only one of them depending on whether you want to use 1) CountVectorizer, 2) TfidfVectorizer, or 3) TfidfVectorizer ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLVtjcnBFOsY"
      },
      "outputs": [],
      "source": [
        "# 1) Only run this cell if you want to use CountVectorizer. Skip the 2 cells below.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(clean_corpus)\n",
        "clean_corpus =  count_vect.transform(clean_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UROpE4t7koqm"
      },
      "outputs": [],
      "source": [
        "# 2) Only run this cell if you want to use TfidfVectorizer. Skip the cell above and below.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# unigram/word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "tfidf_vect.fit(clean_corpus)\n",
        "clean_corpus =  tfidf_vect.transform(clean_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ut8n_uxlDqi"
      },
      "outputs": [],
      "source": [
        "# 3) Only run this cell if you want to use Ngrams. Skip the 2 cells above\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# bigram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2))\n",
        "tfidf_vect_ngram.fit(clean_corpus)\n",
        "clean_corpus =  tfidf_vect_ngram.transform(clean_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "7yxKCwXAF4kr",
        "outputId": "4c585370-0175-40dd-d71a-4545d415d4ec"
      },
      "outputs": [],
      "source": [
        "clean_corpus = pd.DataFrame.sparse.from_spmatrix(clean_corpus)\n",
        "clean_corpus.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlkbQveIgiH2",
        "outputId": "f9c61b09-82d0-41e4-be46-4bc93d532221"
      },
      "outputs": [],
      "source": [
        "import textblob\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "df['adj_count'] = df['Statement'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "df['adv_count'] = df['Statement'].apply(lambda x: check_pos_tag(x, 'adv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "Oe8lPrK_i3GQ",
        "outputId": "dd5a5ea8-7032-429f-e75a-486febb44b1f"
      },
      "outputs": [],
      "source": [
        "result = pd.concat([df, clean_corpus], axis=1)\n",
        "result= result.drop(['Statement', 'preprocessedStatement'], axis=1) \n",
        "result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DamfRMHj2HN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "X_train,X_test, y_train, y_test = train_test_split(result,y, stratify=y, test_size=0.05, random_state=0)\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90J09eHBlX87",
        "outputId": "3c2dae77-49ab-49b2-9041-b24ac09b009a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_lr = LogisticRegression(multi_class='ovr', max_iter=1000) #Insert class_weight = class_weight if training on Meld-dd dataset\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_8QKp7Blbl8"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "model_lr2 = LogisticRegression(max_iter=1000) #Insert class_weight = class_weight if training on Meld-dd dataset\n",
        "\n",
        "model_lr2 = OneVsRestClassifier(model_lr2)\n",
        "\n",
        "model_lr2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr2 = model_lr2.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC9Dgy1LliV5"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model_svm = SVC(decision_function_shape='ovo') #insert class_weight = 'balanced' if training on Meld-dd dataset\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIkcWhXWljE0"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "model_svm2 = SVC() #insert class_weight = 'balanced' if training on Meld-dd dataset\n",
        "\n",
        "model_svm2 = OneVsOneClassifier(model_svm2)\n",
        "\n",
        "model_svm2.fit(X_train, y_train)\n",
        "  \n",
        "y_pred_svm2 = model_svm2.predict(X_test) \n",
        "\n",
        "print(classification_report(y_test, y_pred_svm2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(encoder.inverse_transform([0,1,2,3,4,5,6]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Strategy1-tfidf/countvec.ipynb++.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
