{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "TSbNEiF6XgpT",
        "outputId": "8ba21031-4113-469a-9488-464ec1da8ff8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Statement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>guilt</td>\n",
              "      <td>Once when I was in the cell group (religious a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shame</td>\n",
              "      <td>When I overslept for the second time on the da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>shame</td>\n",
              "      <td>I had not punched a ticket in the bus because ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>disgust</td>\n",
              "      <td>When a man spoke very sexistly in the company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shame</td>\n",
              "      <td>About a dozen girls laughed at me and I was su...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Emotion                                          Statement\n",
              "0    guilt  Once when I was in the cell group (religious a...\n",
              "1    shame  When I overslept for the second time on the da...\n",
              "2    shame  I had not punched a ticket in the bus because ...\n",
              "3  disgust  When a man spoke very sexistly in the company ...\n",
              "4    shame  About a dozen girls laughed at me and I was su..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('LOCAL_PATH_TO_DATASET')\n",
        "df = df[['Emotion','Statement']]\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WNfvaIuQfzOv"
      },
      "outputs": [],
      "source": [
        "def process_text(document):\n",
        "     \n",
        "    # Remove extra white space from text\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "         \n",
        "    # Remove all the special characters from text\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        " \n",
        "    # Remove all single characters from text\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        " \n",
        "    # Word tokenization       \n",
        "    tokens = document.split()\n",
        "      \n",
        "    lemma_txt = [stemmer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Remove Drop words \n",
        "    lemma_no_stop_txt = [word for word in lemma_txt if word not in en_stop]\n",
        "        \n",
        "    tokens = [word for word in tokens if len(word) > 3]\n",
        "                 \n",
        "    clean_txt = ' '.join(lemma_no_stop_txt)\n",
        " \n",
        "    return clean_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0wqv0Kkf0DG",
        "outputId": "233553be-5c3b-4639-922c-2e65c6dc21a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    wa cell group religious activity found almost ...\n",
              "1                overslept second time day examination\n",
              "2    punched ticket bus card ticket collector came ...\n",
              "3               man spoke sexistly company friend mine\n",
              "4          dozen girl laughed wa sure wa nothing wrong\n",
              "Name: preprocessedStatement, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "import re\n",
        "from nltk import WordNetLemmatizer\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')  \n",
        "\n",
        "df['preprocessedStatement'] = df.Statement.apply(process_text)\n",
        "df['preprocessedStatement'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY9OOJz813N5",
        "outputId": "f6dc3b09-4e03-47e5-e56b-05665ea23a3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wa cell group religious activity found almost everyone group read bible daily felt guilty heart',\n",
              " 'overslept second time day examination',\n",
              " 'punched ticket bus card ticket collector came turned forgotten shame felt wa great though wa done purpose',\n",
              " 'man spoke sexistly company friend mine',\n",
              " 'dozen girl laughed wa sure wa nothing wrong']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_corpus = df.preprocessedStatement.tolist()\n",
        "clean_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_cmaZUob4jr7"
      },
      "outputs": [],
      "source": [
        "y = df.pop('Emotion')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For the next 3 cells, run only one of them depending on whether you want to use 1) CountVectorizer, 2) TfidfVectorizer, or 3) TfidfVectorizer ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8PQeJaOfkF7u"
      },
      "outputs": [],
      "source": [
        "# 1) Only run this cell if you want to use CountVectorizer. Skip the 2 cells below.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X_train,X_test, y_train, y_test = train_test_split(clean_corpus,y, stratify=y, test_size=0.05, random_state=0)\n",
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(clean_corpus)\n",
        "\n",
        "#transform the training and validation data using count vectorizer object\n",
        "X_train =  count_vect.transform(X_train)\n",
        "X_test =  count_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IqUgNeP6lk77"
      },
      "outputs": [],
      "source": [
        "# 2) Only run this cell if you want to use TfidfVectorizer. Skip the cell above and below.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X_train,X_test, y_train, y_test = train_test_split(clean_corpus,y, stratify=y, test_size=0.05, random_state=0)\n",
        "\n",
        "# unigram/word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "tfidf_vect.fit(clean_corpus)\n",
        "\n",
        "X_train =  tfidf_vect.transform(X_train)\n",
        "X_test =  tfidf_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n4DaGPR_l_-A"
      },
      "outputs": [],
      "source": [
        "# 3) Only run this cell if you want to use Ngrams. Skip the 2 cells above.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X_train,X_test, y_train, y_test = train_test_split(clean_corpus,y, stratify=y, test_size=0.05, random_state=0)\n",
        "\n",
        "# bigram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2))\n",
        "tfidf_vect_ngram.fit(clean_corpus)\n",
        "X_train =  tfidf_vect_ngram.transform(X_train)\n",
        "X_test =  tfidf_vect_ngram.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ojsXxcglktv0"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le81qKZmkyfF",
        "outputId": "81f6a4d8-e7a3-4da9-a407-ebb88deb5365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.50      0.50        54\n",
            "           1       0.52      0.60      0.56        53\n",
            "           2       0.79      0.81      0.80        54\n",
            "           3       0.54      0.52      0.53        52\n",
            "           4       0.60      0.80      0.69        55\n",
            "           5       0.73      0.61      0.67        54\n",
            "           6       0.68      0.46      0.55        54\n",
            "\n",
            "    accuracy                           0.62       376\n",
            "   macro avg       0.62      0.62      0.61       376\n",
            "weighted avg       0.62      0.62      0.61       376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weight = class_weight.compute_class_weight('balanced'\n",
        "                                               ,np.unique(y)\n",
        "                                               ,y)\n",
        "\n",
        "model_lr = LogisticRegression(multi_class='ovr', max_iter=1000) #Insert class_weight = class_weight if training on Meld-dd dataset\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy8I3tpdbFk9",
        "outputId": "406fea05-de26-4840-e7ce-dd3cae8ccd8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.50      0.50        54\n",
            "           1       0.52      0.60      0.56        53\n",
            "           2       0.79      0.81      0.80        54\n",
            "           3       0.54      0.52      0.53        52\n",
            "           4       0.60      0.80      0.69        55\n",
            "           5       0.73      0.61      0.67        54\n",
            "           6       0.68      0.46      0.55        54\n",
            "\n",
            "    accuracy                           0.62       376\n",
            "   macro avg       0.62      0.62      0.61       376\n",
            "weighted avg       0.62      0.62      0.61       376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "model_lr2 = LogisticRegression(max_iter=1000) #Insert class_weight = class_weight if training on Meld-dd dataset\n",
        "\n",
        "model_lr2 = OneVsRestClassifier(model_lr2)\n",
        "\n",
        "model_lr2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr2 = model_lr2.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_lr2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uVZn7g6bRu7"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model_svm = SVC(decision_function_shape='ovo') #insert class_weight = 'balanced' if training on Meld-dd dataset\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn0SQqc0mcCe"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "model_svm2 = SVC() #insert class_weight = 'balanced' if training on Meld-dd dataset\n",
        "\n",
        "model_svm2 = OneVsOneClassifier(model_svm2)\n",
        "\n",
        "model_svm2.fit(X_train, y_train)\n",
        "  \n",
        "y_pred_svm2 = model_svm2.predict(X_test) \n",
        "\n",
        "print(classification_report(y_test, y_pred_svm2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk2fJBMgp_uH",
        "outputId": "8fd56c68-8853-4dc6-d7bb-358745c4e196"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(encoder.inverse_transform([0,1,2,3,4,5,6]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Strategy1-tfidf/countvec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
