{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX47fH43vkrg",
        "outputId": "832d3bd5-48f2-4990-a77e-9e696e6214c5"
      },
      "outputs": [],
      "source": [
        "#Model architecture inspired by Y. Kim, \"Convolutional Neural Networks for Sentence Classification\", 2014\n",
        "#!pip install fasttext\n",
        "\n",
        "import numpy as np\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "    \n",
        "# Download the cc.en.300.bin English model beforehand from FastText website\n",
        "ft = fasttext.load_model('LOCAL_PATH_TO_cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCBRLT1Evy91"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "qpxjjQPGv8f3",
        "outputId": "6af5b588-b974-403c-92de-f3b0105250e0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('LOCAL_PATH_TO_DATASET')\n",
        "df = df[['Emotion','Statement']]\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTO3p2SwTeK"
      },
      "outputs": [],
      "source": [
        "def process_text(document):\n",
        "     \n",
        "    # Remove extra white space from text\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "         \n",
        "    # Remove all the special characters from text\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        " \n",
        "    # Remove all single characters from text\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        " \n",
        "    # Word tokenization       \n",
        "    tokens = document.split()\n",
        "        \n",
        "    tokens = [word for word in tokens if len(word) > 2]\n",
        "                 \n",
        "    clean_txt = ' '.join(tokens)\n",
        " \n",
        "    return clean_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "AR1-uzpzwWDh",
        "outputId": "d785a929-a771-4694-ad22-79dcf3e3c86b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# For sentence tokenization\n",
        "#nltk.download('punkt')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "df['preprocessedStatement'] = df.Statement.apply(process_text)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31MoiSJowZKX"
      },
      "outputs": [],
      "source": [
        "max_length = df.preprocessedStatement.apply(lambda x: len(x.split())).max()\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(df['preprocessedStatement'] )\n",
        "vocab_size = len(t.word_index) + 1\n",
        "encoded_tweets = t.texts_to_sequences(df['preprocessedStatement'] )\n",
        "X = pad_sequences(encoded_tweets, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24OTyDQZwjh7"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = ft.get_word_vector(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZxr4Au9wsyP",
        "outputId": "81678517-7e98-4463-faa1-bd0aa319b0fe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "# Encode labels in column 'Emotion'. \n",
        "df['Emotion'] = le.fit_transform(df['Emotion']) \n",
        "y = df.pop('Emotion')\n",
        "#print(y)\n",
        "y_new = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "print(y_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbubcM4NYvWm",
        "outputId": "e2d4e941-792d-41a6-c254-9787285d9cb1"
      },
      "outputs": [],
      "source": [
        "list(le.inverse_transform([0,1,2,3,4,5,6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSRP-jDaYzez",
        "outputId": "306ac2f0-eba1-4570-89a8-a2955fda1ad6"
      },
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2 options to handle imbalanced dataset: class_weight or focal loss\n",
        "class_weight = {0: 6, 1: 22, 2: 30, 3: 1, 4: 2, 5: 7, 6: 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJxeP5pNwtfE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_new, test_size=0.05, stratify=y_new)\n",
        "\n",
        "x_val = x_train[:100]\n",
        "y_val = y_train[:100]\n",
        "x_train = x_train[100:]\n",
        "y_train = y_train[100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import activations\n",
        "\n",
        "def focal_loss(gamma=2., alpha=4., from_logits=False):\n",
        "\n",
        "    gamma = float(gamma)\n",
        "    alpha = float(alpha)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Focal loss for multi-classification\n",
        "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
        "        Notice: y_pred is probability after softmax if from_logits is False.\n",
        "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
        "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
        "        Focal Loss for Dense Object Detection\n",
        "        https://arxiv.org/abs/1708.02002\n",
        "\n",
        "        Arguments:\n",
        "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
        "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
        "\n",
        "        Keyword Arguments:\n",
        "            gamma {float} -- (default: {2.0})\n",
        "            alpha {float} -- (default: {4.0})\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- loss.\n",
        "        \"\"\"\n",
        "        epsilon = 1.e-9\n",
        "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        if from_logits:\n",
        "            y_pred = activations.softmax(y_pred)\n",
        "\n",
        "        model_out = tf.add(y_pred, epsilon)\n",
        "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
        "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
        "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
        "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
        "        return tf.reduce_mean(reduced_fl)\n",
        "    return focal_loss_fixed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LUOHJNTwv2h",
        "outputId": "be80b176-09ba-4711-d616-fda7dd326e71"
      },
      "outputs": [],
      "source": [
        "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_ft = Sequential()\n",
        "model_ft.add(Embedding(vocab_size, 300, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
        "model_ft.add(Conv1D(128, 5, activation='relu'))\n",
        "model_ft.add(GlobalMaxPooling1D())\n",
        "model_ft.add(Dense(64, activation='relu'))\n",
        "model_ft.add(Dense(7, activation='softmax'))\n",
        "#model_ft.compile(loss=focal_loss(alpha=1), optimizer='adam', metrics=['accuracy']) #use if you are using focal loss\n",
        "model_ft.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #use if you are using class_weight\n",
        "model_ft.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_ft,show_shapes= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY0gzeCnw1sE",
        "outputId": "32324b46-d267-4cd5-fab9-0a84dc9f5442"
      },
      "outputs": [],
      "source": [
        "model_ft.fit(x_train, y_train, epochs = 20, validation_data=(x_val, y_val), callbacks=callback) #insert class_weight=class_weight if using class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKtEf7aGw3sp"
      },
      "outputs": [],
      "source": [
        "y_pred = model_ft.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8DRcGwiw4ig"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, 1)\n",
        "y_test = np.argmax(y_test, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENJMkBZzw8TC",
        "outputId": "57a79041-7429-4e21-bc14-1b20084c7f9e"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hi6yCFTYi8o",
        "outputId": "34a72e82-27db-418b-8c69-38f9f82c025d"
      },
      "outputs": [],
      "source": [
        "list(le.inverse_transform([0,1,2,3,4,5,6]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN-Fasttext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
