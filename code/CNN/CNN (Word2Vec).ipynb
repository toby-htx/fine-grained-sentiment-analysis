{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q665_SVNNnc"
      },
      "outputs": [],
      "source": [
        "#Model architecture inspired by Y. Kim, \"Convolutional Neural Networks for Sentence Classification\", 2014\n",
        "import gensim.downloader as api\n",
        "\n",
        "info = api.info()  # show info about available models/datasets\n",
        "w2v = api.load(\"word2vec-google-news-300\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gukWzBodNOuV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers.embeddings import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "U7EHfVtKNRFC",
        "outputId": "963000b5-3a06-4fbf-d329-b0486f8d3199"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Statement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>guilt</td>\n",
              "      <td>Once when I was in the cell group (religious a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shame</td>\n",
              "      <td>When I overslept for the second time on the da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>shame</td>\n",
              "      <td>I had not punched a ticket in the bus because ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>disgust</td>\n",
              "      <td>When a man spoke very sexistly in the company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shame</td>\n",
              "      <td>About a dozen girls laughed at me and I was su...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Emotion                                          Statement\n",
              "0    guilt  Once when I was in the cell group (religious a...\n",
              "1    shame  When I overslept for the second time on the da...\n",
              "2    shame  I had not punched a ticket in the bus because ...\n",
              "3  disgust  When a man spoke very sexistly in the company ...\n",
              "4    shame  About a dozen girls laughed at me and I was su..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.read_csv('LOCAL_PATH_TO_DATASET')\n",
        "df = df[['Emotion','Statement']]\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjwGfh88NUEX"
      },
      "outputs": [],
      "source": [
        "def process_text(document):\n",
        "     \n",
        "    # Remove extra white space from text\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "         \n",
        "    # Remove all the special characters from text\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        " \n",
        "    # Remove all single characters from text\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        " \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        " \n",
        "    # Word tokenization       \n",
        "    tokens = document.split()\n",
        "\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "                 \n",
        "    clean_txt = ' '.join(tokens)\n",
        " \n",
        "    return clean_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "v0LZf71lNWY5",
        "outputId": "03aa26a6-bb41-4a0d-ea5d-a2363875b02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Statement</th>\n",
              "      <th>preprocessedStatement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>guilt</td>\n",
              "      <td>Once when I was in the cell group (religious a...</td>\n",
              "      <td>cell group religious activity found almost eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shame</td>\n",
              "      <td>When I overslept for the second time on the da...</td>\n",
              "      <td>overslept second time day examination</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>shame</td>\n",
              "      <td>I had not punched a ticket in the bus because ...</td>\n",
              "      <td>punched ticket bus card ticket collector came ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>disgust</td>\n",
              "      <td>When a man spoke very sexistly in the company ...</td>\n",
              "      <td>man spoke sexistly company friends mine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shame</td>\n",
              "      <td>About a dozen girls laughed at me and I was su...</td>\n",
              "      <td>dozen girls laughed sure nothing wrong</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Emotion  ...                              preprocessedStatement\n",
              "0    guilt  ...  cell group religious activity found almost eve...\n",
              "1    shame  ...              overslept second time day examination\n",
              "2    shame  ...  punched ticket bus card ticket collector came ...\n",
              "3  disgust  ...            man spoke sexistly company friends mine\n",
              "4    shame  ...             dozen girls laughed sure nothing wrong\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# For sentence tokenization\n",
        "#nltk.download('punkt')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "import re\n",
        "from nltk import WordNetLemmatizer\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('wordnet')\n",
        "df['preprocessedStatement'] = df.Statement.apply(process_text)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmYXG429NW0b"
      },
      "outputs": [],
      "source": [
        "max_length = df.preprocessedStatement.apply(lambda x: len(x.split())).max()\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(df['preprocessedStatement'] )\n",
        "vocab_size = len(t.word_index) + 1\n",
        "encoded_text = t.texts_to_sequences(df['preprocessedStatement'] )\n",
        "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVopMP3bNYcg"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    try:\n",
        "      embedding_vector = w2v[word]\n",
        "    except KeyError:\n",
        "      pass\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvT596tUNk3G",
        "outputId": "85bf7d93-5540-486f-b41f-913f5f4e58e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "# Encode labels in column 'Emotion'. \n",
        "df['Emotion'] = le.fit_transform(df['Emotion']) \n",
        "y = df.pop('Emotion')\n",
        "y_new = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "print(y_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X4mjobQZKjs",
        "outputId": "73261d77-6291-436a-ebd3-01928fb1db6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(le.inverse_transform([0,1,2,3,4,5,6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxpTu6hHZLNa",
        "outputId": "1db9e197-9c06-4481-ad66-d56d6550b830"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4    1092\n",
              "5    1082\n",
              "0    1079\n",
              "2    1076\n",
              "6    1071\n",
              "1    1066\n",
              "3    1050\n",
              "Name: Emotion, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjylKkcBZOdp"
      },
      "outputs": [],
      "source": [
        "# 2 options to handle imbalanced dataset: class_weight or focal loss\n",
        "class_weight = {0: 6, 1: 22, 2: 30, 3: 1, 4: 2, 5: 7, 6: 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfpT4xsDNmVI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_new, test_size=0.05, stratify=y_new)\n",
        "\n",
        "x_val = x_train[:100]\n",
        "y_val = y_train[:100]\n",
        "x_train = x_train[100:]\n",
        "y_train = y_train[100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import activations\n",
        "\n",
        "def focal_loss(gamma=2., alpha=4., from_logits=False):\n",
        "\n",
        "    gamma = float(gamma)\n",
        "    alpha = float(alpha)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Focal loss for multi-classification\n",
        "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
        "        Notice: y_pred is probability after softmax if from_logits is False.\n",
        "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
        "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
        "        Focal Loss for Dense Object Detection\n",
        "        https://arxiv.org/abs/1708.02002\n",
        "\n",
        "        Arguments:\n",
        "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
        "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
        "\n",
        "        Keyword Arguments:\n",
        "            gamma {float} -- (default: {2.0})\n",
        "            alpha {float} -- (default: {4.0})\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- loss.\n",
        "        \"\"\"\n",
        "        epsilon = 1.e-9\n",
        "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        if from_logits:\n",
        "            y_pred = activations.softmax(y_pred)\n",
        "\n",
        "        model_out = tf.add(y_pred, epsilon)\n",
        "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
        "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
        "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
        "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
        "        return tf.reduce_mean(reduced_fl)\n",
        "    return focal_loss_fixed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vEFLvwtNo6t",
        "outputId": "b665c257-b1d1-4592-88e3-a257aa4514b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 78, 300)           2651700   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 74, 128)           192128    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 455       \n",
            "=================================================================\n",
            "Total params: 2,852,539\n",
            "Trainable params: 200,839\n",
            "Non-trainable params: 2,651,700\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_w2v = Sequential()\n",
        "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_w2v.add(Embedding(vocab_size, 300, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
        "model_w2v.add(Conv1D(128, 5, activation='relu'))\n",
        "model_w2v.add(GlobalMaxPooling1D())\n",
        "model_w2v.add(Flatten())\n",
        "model_w2v.add(Dense(64, activation='relu'))\n",
        "model_w2v.add(Dense(7, activation='softmax'))\n",
        "#model_w2v.compile(loss=focal_loss(alpha=1), optimizer='adam', metrics=['accuracy']) #use if you are using focal loss\n",
        "model_w2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #use if you are using class_weight\n",
        "model_w2v.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_w2v,show_shapes= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp011b_KOA9B",
        "outputId": "83be06d0-e6f1-4203-e237-e4e98aa586e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "220/220 [==============================] - 14s 7ms/step - loss: 1.4690 - accuracy: 0.4659 - val_loss: 1.2436 - val_accuracy: 0.4900\n",
            "Epoch 2/20\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.8988 - accuracy: 0.6949 - val_loss: 1.2309 - val_accuracy: 0.5600\n",
            "Epoch 3/20\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.5471 - accuracy: 0.8347 - val_loss: 1.2813 - val_accuracy: 0.4900\n",
            "Epoch 4/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2742 - accuracy: 0.9369 - val_loss: 1.4319 - val_accuracy: 0.5100\n",
            "Epoch 5/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1291 - accuracy: 0.9756 - val_loss: 1.5122 - val_accuracy: 0.5200\n",
            "Epoch 6/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0719 - accuracy: 0.9879 - val_loss: 1.7128 - val_accuracy: 0.5300\n",
            "Epoch 7/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0495 - accuracy: 0.9903 - val_loss: 1.7674 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0445 - accuracy: 0.9922 - val_loss: 1.8241 - val_accuracy: 0.5600\n",
            "Epoch 9/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0390 - accuracy: 0.9935 - val_loss: 1.9007 - val_accuracy: 0.5400\n",
            "Epoch 10/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0377 - accuracy: 0.9940 - val_loss: 2.0632 - val_accuracy: 0.4800\n",
            "Epoch 11/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0368 - accuracy: 0.9935 - val_loss: 2.0600 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0379 - accuracy: 0.9932 - val_loss: 2.0615 - val_accuracy: 0.5100\n",
            "Epoch 13/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0351 - accuracy: 0.9943 - val_loss: 2.1230 - val_accuracy: 0.5100\n",
            "Epoch 14/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0300 - accuracy: 0.9936 - val_loss: 2.1727 - val_accuracy: 0.5100\n",
            "Epoch 15/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0302 - accuracy: 0.9943 - val_loss: 2.3151 - val_accuracy: 0.5100\n",
            "Epoch 16/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0431 - accuracy: 0.9888 - val_loss: 3.0217 - val_accuracy: 0.4600\n",
            "Epoch 17/20\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2012 - accuracy: 0.9368 - val_loss: 2.7383 - val_accuracy: 0.4600\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07d305c190>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w2v.fit(x_train, y_train, epochs = 20, validation_data=(x_val, y_val), callbacks=[callback]) #insert class_weight=class_weight if using class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI2dXKgROBBZ"
      },
      "outputs": [],
      "source": [
        "y_pred = model_w2v.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUve-AQOERF"
      },
      "outputs": [],
      "source": [
        "y_pred_clean = np.argmax(y_pred, 1)\n",
        "y_test_clean = np.argmax(y_test, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZzCQrIqOFyZ",
        "outputId": "6643beb2-d8d9-4c6b-ee5d-95c9708b0107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.33      0.35        54\n",
            "           1       0.56      0.57      0.56        53\n",
            "           2       0.56      0.78      0.65        54\n",
            "           3       0.43      0.42      0.43        52\n",
            "           4       0.63      0.82      0.71        55\n",
            "           5       0.79      0.56      0.65        54\n",
            "           6       0.61      0.43      0.50        54\n",
            "\n",
            "    accuracy                           0.56       376\n",
            "   macro avg       0.56      0.56      0.55       376\n",
            "weighted avg       0.56      0.56      0.55       376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test_clean, y_pred_clean))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN-W2V.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
